#!/bin/bash

# HTTPSé“¾æ¥æ›´æ–°è„šæœ¬
# ç”¨äºå°†äº§å“ä¸»é¡µä¸­çš„HTTPé“¾æ¥æ›´æ–°ä¸ºHTTPS

echo "ğŸ”„ å¼€å§‹æ›´æ–°HTTPé“¾æ¥ä¸ºHTTPS..."

# å®šä¹‰é™æ€é¡µé¢ç›®å½•
STATIC_DIR="static-pages/pdhtml"

if [ ! -d "$STATIC_DIR" ]; then
    echo "âŒ é™æ€é¡µé¢ç›®å½•ä¸å­˜åœ¨: $STATIC_DIR"
    exit 1
fi

echo "ğŸ“ æ‰«æç›®å½•: $STATIC_DIR"

# ç»Ÿè®¡å˜é‡
updated_files=0
total_files=0

# æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶å¹¶æ›´æ–°é“¾æ¥
find "$STATIC_DIR" -name "*.html" -type f | while read -r file; do
    total_files=$((total_files + 1))
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«HTTPé“¾æ¥
    if grep -q "http://productmindai.com" "$file"; then
        echo "ğŸ”§ æ›´æ–°æ–‡ä»¶: $file"
        
        # å¤‡ä»½åŸæ–‡ä»¶
        cp "$file" "$file.backup"
        
        # æ›¿æ¢HTTPä¸ºHTTPS
        sed -i 's|http://productmindai.com|https://productmindai.com|g' "$file"
        
        updated_files=$((updated_files + 1))
        echo "âœ… å·²æ›´æ–°: $file"
    fi
done

echo ""
echo "ğŸ“Š æ›´æ–°å®Œæˆç»Ÿè®¡:"
echo "   æ€»æ–‡ä»¶æ•°: $(find "$STATIC_DIR" -name "*.html" -type f | wc -l)"
echo "   å·²æ›´æ–°æ–‡ä»¶: $updated_files"

# æ£€æŸ¥æ˜¯å¦æœ‰å¤‡ä»½æ–‡ä»¶
backup_count=$(find "$STATIC_DIR" -name "*.backup" -type f | wc -l)
if [ $backup_count -gt 0 ]; then
    echo "ğŸ“¦ å¤‡ä»½æ–‡ä»¶æ•°: $backup_count"
    echo "ğŸ’¡ å¦‚éœ€æ¢å¤ï¼Œå¤‡ä»½æ–‡ä»¶åç¼€ä¸º .backup"
fi

echo ""
echo "ğŸ‰ HTTPSé“¾æ¥æ›´æ–°å®Œæˆï¼"

# éªŒè¯æ›´æ–°ç»“æœ
echo "ğŸ” éªŒè¯æ›´æ–°ç»“æœ..."
remaining_http=$(find "$STATIC_DIR" -name "*.html" -type f -exec grep -l "http://productmindai.com" {} \; | wc -l)

if [ $remaining_http -eq 0 ]; then
    echo "âœ… æ‰€æœ‰HTTPé“¾æ¥å·²æˆåŠŸæ›´æ–°ä¸ºHTTPS"
else
    echo "âš ï¸  ä»æœ‰ $remaining_http ä¸ªæ–‡ä»¶åŒ…å«HTTPé“¾æ¥ï¼Œè¯·æ£€æŸ¥"
    find "$STATIC_DIR" -name "*.html" -type f -exec grep -l "http://productmindai.com" {} \;
fi

echo ""
echo "ğŸ”— æµ‹è¯•HTTPSè®¿é—®..."
if curl -s -I https://productmindai.com | grep -q "200 OK"; then
    echo "âœ… HTTPSè®¿é—®æ­£å¸¸"
else
    echo "âš ï¸  HTTPSè®¿é—®å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥é…ç½®"
fi 